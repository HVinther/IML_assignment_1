---
title: "IML assignment 1"
author: "Emil Skinders√∏, Christopher Fjeldberg Jensen and Hans Vinther-Larsen"
date: "2024-02-26"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, message=FALSE,warning=FALSE}
library(tidyverse)
source("utils.R")
source("libraries.R")
source("extra_pipeops.R")
loadData()
```
# Introduction

In this project we use different machinelearning models to predict the claim amount for individual customers of an insurance company. Particularly, we are interested in predicting the claim amount for customers who have had insurance for at least a whole year (i.e exposure=1). Aside from choosing suitable factor encodings and models, we are interested in optimizing our choices such that the predictions are best at exposure=1. To this end, we employ weightings and custom measures (see the following paragraphs).
can be found on github at https://github.com/HVinther/IML_assignment_1

# Data exploration, encoding and weighting

The dataset is a subset of the freMPL datasets from the CASdatasets library. It is split into training and testing with 80% of the data being used for training. 

```{r}
head(freMPL)
```

The data consists of some variables such as vehicle price and age, as well as social class, which can be encoded either as factors or as numerical. Additionally for categorical variables, we try with dummy and target encoding.

Since we are interested in optimizing best for exposure=1, we make some weight functions such that individuals whose exposure is closer to 1 are more important that individuals with low exposure.

## Encoding of dates and handling of missing values

Two <Date> columns are present in the dataset, RecordBeg and RecordEnd representing the beginning and end dates of recording. Several choices have to be made in the handling of these variables. First of, the `as_task` functions from `mlr3`cannot handle the <Date> datatype. A quick reencoding as `POSIXct`overcomes this initial hurdle - this will however need further encoding, as most models cannot handle the `POSIXct` class directly, but this can now be done inside a GraphLearner object. 

Before settling on an encoding strategy a second issue has to be considered. The RecordEnd variable has an abundance of missing values, in both the full dataset and in the training dataset about $46\%$ of the observations of this variable are missing. In particular these seem to be a result of right censoring, as all observations are within the year 2004, these particular observations are those, where the record has not ended within the year. Imputing these variables based on the nonmissing would expectedly result in an under estimation of their values.

We suggest and investigate the following encoding strategies for these variables:

A numerical encoding,imputing using lastest observed date and indicating censoring

```{r, eval = FALSE}
# Interpretes RecordEnd as being right censored.
# Thus imputes using the maximal observed time, 
# and adds column indicating censoring.
po_RecEnd_rc<-po(
  "mutate",
  mutation = list(
    RecordEnd = ~as.numeric(ifelse(is.na(RecordEnd),
                                   max(na.omit(RecordEnd)),
                                   RecordEnd)),
    RecordEnd_censored = ~is.na(RecordEnd)
  ),
  id = "RecEnd_rc"
)

# Reencodes RecordBeg (POSIXct) to numeric
po_RecBeg_num<-po(
  "mutate",
  mutation = list(
    RecordBeg = ~as.numeric(RecordBeg)
  ),
  id = "RecBeg_num"
)
```


## Encoding of remaining covariates

## Weighting

### By interest
As where are actually only interested in approximating $\mathbb{E}[|\text{Exposure} = 1]$, we ...
$f:[0,1]\longrightarrow\mathbb{R}_{>=0}$

While weighting is normally done to fit models on imbalanced datasets, the same method should allow us to prioritize a precise fit on the ... of interest while hopefully still extracting some information from the remaining dataset.

$$w_i = f_{k,l}(x_i):= \frac{\exp((x_i-l)\cdot k)}{1+\exp((x_i-l)\cdot k)} $$

```{r, echo=FALSE}
x<- seq(from = 0,
        to = 1,
        length.out = 101)

sigmoid <- function(x,k = 12){
  inner <- exp((x-0.5)*k)
  return(inner/(1+inner))
}

ggplot()+
  geom_line(
    aes(x = x,
        y = sigmoid(x))
  )
```
This weighting ... for $k=12$
```{r, eval = FALSE}
## Creates a weighting using exposure and sets it as the weigthing
po_add_weighting<-
  po("mutate",
     id = "create_weight",
     mutation =list(
       weights =~exp((Exposure-0.5)*12)/(1+exp((Exposure-0.5)*12))
       ))%>>%
  po("colroles",
     id = "set_weight",
     new_role = list(
       weights = "weight"
       ))
```

### By frequency

An alternative weighting approach more enlign with standard methods would be to weight to tackle the imbaling with respect to the sign of `ClaimAmount`. A quick 

```{r}
train |> 
  group_by(sign(ClaimAmount))|>
  summarize(rel_freq = n()/nrow(train))|>
  mutate(reciprocal_rel_freq = 1/rel_freq)
```
Thus the vast majority of observations have `ClaimAmount` equal to zero. bla... bla.. reciprocal frequency

# Choosing Learners and Encoding

When implementing our algorithm we decided to focus our attention on three different learners: Glmnet, Gradient boost and random forest. We also considered trying with a k-neareast neighbours learner, but we deemed that the dimension was too large
for it to get a good result.

Also to choose our best performing algorithm we decided on the approach of first comparing encoding strategies within the same and learner and choose the best performing algorithm for each learner, and then compare our best algorithm along with featureless learner. When deciding on which algorithm is the best performing we resample with 5-fold cross-validation and then we plot the mean squared error. 

## Encoding strategy

For a more fair comparison of our algorithms we decided on a common encoding strategy across all the learners. We begin with some simple baseline graphs, with no weighting and no custom encoding. Next we create some graphs with our own encoder for the `SocioCateg` variable.

```{r, eval=FALSE}
Dummy_lrn <- po("encode") %>>% po_RecBeg_num %>>% po_RecEnd_rc %>>% po("scale") %>>% lrn_obj |> as_learner()
Target_lrn <- po("encodeimpact") %>>% po_RecBeg_num %>>% po_RecEnd_rc %>>% po("scale") %>>% lrn_obj |> as_learner()
Dummy_lrn_custom <- po_VehAge_num %>>% po_VehPrice_int %>>% po_SocCat_int %>>% po("encode") %>>% po_RecBeg_num %>>% po_RecEnd_rc %>>% po("scale") %>>% lrn_obj |> as_learner()
Target_lrn_custom <- po_VehAge_num %>>% po_VehPrice_int %>>% po_SocCat_int %>>% po("encodeimpact") %>>% po_RecBeg_num %>>% po_RecEnd_rc %>>% po("scale") %>>% lrn_obj |> as_learner()
```

We have chosen these encoding strategies because dummy and target encoding are two commonly used encoding strategies, so it is relevant to compare these. We also deem that turning the categorical features into numerical ones could have an effect, in a the sense that assuming a linear effect of these features could improve performance.

For testing purposes we then compare these on the normal training task, and also on tasks with either our interest weighting or our frequency weighting.

```{r,eval=FALSE}
learner_BM <- benchmark_grid(
  tasks = list(train_task, add_weight(train_task,weighting = "interest"),add_weight(train_task,weighting = "frequency"),add_weight(train_task)),
  learners = list(Dumm_lrn, Target_lrn,Dummy_lrn_SocCat,Target_lrn_SocCat),
  resamplings = rsmp("cv",folds=3)
)|> benchmark()
```


## Glmnet

```{r, fig.width = 16, fig.height= 9, echo=FALSE}
readRDS("GnG_score_cv5.rds") |> 
  pivot_longer(cols = c("regr.mse","mse_inter"),
               names_to = "measure")|>
  ggplot()+
  geom_boxplot(
    aes(y = value,
        x = learner,
        fill = encoding),
    position = position_dodge2(preserve = "single"))+
  facet_grid(measure ~ task_id, scales = "free_y")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom")

```


## Gradient Boost

We wanted to work with XGBoost because it is a widely used ML-model, which often has fairly strong predictive power. 

There are a lot of hyperparameters to tune, and we select two of them. The first is the choice of booster, particularly 'dart', 'gbtree' and 'gblinear' where the first two are tree-based and the third is a linear function approximation. The second is the max_depth of the tree, in the case that the learner is tree-based, and this is tested between 1 and 500 at a step-length of 50. We use 5-fold cross-validation for hyper-parameter tuning.

In order to make a choice of weighting and encoding, as described above, we use all 4 combinations of weighting (no weight, frequency, interest and both) and all 4 factors (dummy,target,numeric dummy and numeric target). The results are as follows:

```{r}
bench_mark = readRDS("xgbench")
plot(bench_mark)

ggplot(bench_mark$aggregate(list(msr("time_train"),msr("regr.mse"))))+
  geom_point(mapping=aes(x=time_train,y=regr.mse,colour=learner_id,shape=task_id))+
  geom_hline(mapping=aes(yintercept=regr.mse,colour=learner_id,linetype=task_id))
```
Where the first plot is a histogram of the mse at the different folds, and the second plot illustrates the average mse for the different combinations, with training time in seconds.

It can be seen that dummy encoding generally gives higher run-time. More importantly, dummy also tends to result in lower average mse, but the variance is comparable across all encoding strategies according to the boxplot. We can see both a lower mean and variance in the interest-weighting strategy, although the median is higher here. These values should be taken with a grain of salt since it is only based on 5 fold cv and has no chance of converging on representable values.

## Random Forest

We choose to do a random forest algorithm because we wanted to implement a tree based method. Since we are interested in the predictive performance of our models we chose to do a random forest implementation since it should have better performance than a decision tree, and should have lower variance than bagging.

The random forest implementation has some well-performing standard hyper-parameters, so we deemed it not necessary to tune the hyper parameters. 

```{r}
load("RforestBM5.RData")
plot(rforest_BM5)

ggplot(rforest_BM5$aggregate(list(msr("regr.mse"),
                                 msr("time_train"))))+
  geom_point(mapping = aes(x=time_train, y=regr.mse, color= learner_id, shape = task_id))+
  geom_hline(mapping = aes(yintercept = regr.mse, color = learner_id),
             linetype = "dashed")+
  xlab("time")+
  ylab("Mean Squared Error")
```
So from this plot we note that target encoding outperforms dummy encoding when using it on our random forest learner. Furthermore we note that our own encodings also performs better than without, but the benefit is smaller for when using it with target encoding. It should also be noted that our encodings were computationally faster with dummy encoding but slower for target encoding. But overall we would conclude that our custom target encoding is the best performing encoding when using a random forest learner.


# Comparing learners

## Custom measure
In order to evaluate which algorithm has best predictive performance, we evaluate the MSE only at the data where exposure=1. The idea being that standard MSE also awards good predictions at lower exposures, but we are not interested in having good predictive performance at these levels.

## Comparison
=======
```{r}
rforest_BM52 <- rforest_BM5$clone()
rforest_BM52$filter(learner_ids = list("dmy_Rfor","dmy_Rfor_cus","Trgt_Rfor","Trgt_Rfor_cus"))
```

```{r}
plot(rforest_BM52)

ggplot(rforest_BM52$aggregate(list(msr("regr.mse"),
                                 msr("time_train"))))+
  geom_point(mapping = aes(x=time_train, y=regr.mse, color= learner_id, shape = task_id))+
  geom_hline(mapping = aes(yintercept = regr.mse, color = learner_id),
             linetype = "dashed")+
  xlab("time")+
  ylab("Mean Squared Error")
```



